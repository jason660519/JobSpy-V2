#!/usr/bin/env python3
"""Seekçˆ¬èŸ²ETL Pipelineå¯¦éš›æ¸¬è©¦è…³æœ¬

é€™å€‹è…³æœ¬å°‡å¯¦éš›é‹è¡ŒSeekçˆ¬èŸ²çš„å®Œæ•´ETLæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. åŸå§‹æ•¸æ“šæŠ“å–éšæ®µ - çˆ¬å–Seekè·ç¼ºè³‡æ–™ä¸¦å­˜å„²åˆ°MinIO raw-dataæ¡¶
2. AIè§£æè™•ç†éšæ®µ - ä½¿ç”¨OpenAIè§£æåŸå§‹æ•¸æ“šä¸¦å­˜å„²åˆ°ai-processedæ¡¶
3. æ•¸æ“šæ¸…ç†éšæ®µ - æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼ä¸¦å­˜å„²åˆ°cleaned-dataæ¡¶
4. æ•¸æ“šåº«è¼‰å…¥éšæ®µ - å°‡æ¸…ç†å¾Œæ•¸æ“šè¼‰å…¥PostgreSQL
5. CSVå°å‡ºéšæ®µ - å¾æ•¸æ“šåº«å°å‡ºCSVæª”æ¡ˆ

ä¸¦é©—è­‰æ–‡ä»¶æ˜¯å¦å­˜æ”¾åœ¨æ­£ç¢ºä½ç½®ã€‚
"""

import asyncio
import os
import sys
import json
import csv
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import structlog

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è¨­ç½®ç’°å¢ƒè®Šé‡ï¼ˆå¦‚æœ.envæ–‡ä»¶å­˜åœ¨ï¼‰
from dotenv import load_dotenv
load_dotenv()

# è¨­ç½®æ—¥èªŒ
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger(__name__)


class SeekETLRunner:
    """Seek ETL Pipeline å¯¦éš›é‹è¡Œå™¨
    
    è² è²¬åŸ·è¡Œå®Œæ•´çš„ETLæµç¨‹ä¸¦é©—è­‰æ¯å€‹éšæ®µçš„è¼¸å‡ºã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ETLé‹è¡Œå™¨"""
        self.logger = logger.bind(component="SeekETLRunner")
        
        # æ¸¬è©¦é…ç½®
        self.test_query = "software engineer"
        self.test_location = "Sydney"
        self.test_limit = 3  # é™åˆ¶æ¸¬è©¦æ•¸é‡ä»¥ç¯€çœè³‡æº
        
        # æ™‚é–“æˆ³ç”¨æ–¼æ–‡ä»¶å‘½å
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ETLéšæ®µç‹€æ…‹è¿½è¹¤
        self.etl_status = {
            "stage_1_raw_data": {"completed": False, "files": [], "count": 0, "errors": []},
            "stage_2_ai_processed": {"completed": False, "files": [], "count": 0, "errors": []},
            "stage_3_cleaned_data": {"completed": False, "files": [], "count": 0, "errors": []},
            "stage_4_db_loaded": {"completed": False, "records": 0, "errors": []},
            "stage_5_csv_exported": {"completed": False, "files": [], "count": 0, "errors": []}
        }
        
        # åˆå§‹åŒ–çµ„ä»¶ï¼ˆå»¶é²åŠ è¼‰ï¼‰
        self.seek_adapter = None
        self.minio_client = None
        self.storage_service = None
        
    async def initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ETLçµ„ä»¶"""
        try:
            self.logger.info("æ­£åœ¨åˆå§‹åŒ–ETLçµ„ä»¶...")
            
            # å°å…¥ä¸¦åˆå§‹åŒ–Seeké©é…å™¨
            from crawler_engine.platforms.seek import SeekAdapter, create_seek_config
            from crawler_engine.platforms.base import SearchRequest, SearchMethod
            
            self.seek_adapter = SeekAdapter(create_seek_config())
            self.logger.info("Seeké©é…å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # å°å…¥ä¸¦åˆå§‹åŒ–MinIOå®¢æˆ¶ç«¯
            try:
                from backend.app.core.minio_client import get_minio_client
                self.minio_client = await get_minio_client()
                self.logger.info("MinIOå®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ")
            except ImportError as e:
                self.logger.warning(f"ç„¡æ³•å°å…¥MinIOå®¢æˆ¶ç«¯: {e}ï¼Œå°‡ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å­˜å„²")
                self.minio_client = None
            
            # å°å…¥ä¸¦åˆå§‹åŒ–å­˜å„²æœå‹™
            try:
                from backend.app.services.storage_service import StorageService
                self.storage_service = StorageService()
                self.logger.info("å­˜å„²æœå‹™åˆå§‹åŒ–å®Œæˆ")
            except ImportError as e:
                self.logger.warning(f"ç„¡æ³•å°å…¥å­˜å„²æœå‹™: {e}")
                self.storage_service = None
            
            return True
            
        except Exception as e:
            self.logger.error(f"çµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            return False
    
    async def run_stage_1_raw_data_extraction(self) -> Dict[str, Any]:
        """éšæ®µ1ï¼šåŸå§‹æ•¸æ“šæŠ“å–
        
        Returns:
            Dict[str, Any]: éšæ®µåŸ·è¡Œçµæœ
        """
        stage_result = {
            "stage": "åŸå§‹æ•¸æ“šæŠ“å–",
            "success": False,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "data_extracted": 0,
            "files_created": [],
            "minio_stored": False,
            "errors": []
        }
        
        try:
            self.logger.info("é–‹å§‹éšæ®µ1ï¼šåŸå§‹æ•¸æ“šæŠ“å–")
            
            # å‰µå»ºæœç´¢è«‹æ±‚
            from crawler_engine.platforms.base import SearchRequest, SearchMethod
            
            search_request = SearchRequest(
                query=self.test_query,
                location=self.test_location,
                limit=self.test_limit,
                page=1
            )
            
            # åŸ·è¡Œæœç´¢
            search_result = await self.seek_adapter.search_jobs(
                search_request, 
                SearchMethod.WEB_SCRAPING
            )
            
            if search_result.success and search_result.jobs:
                stage_result["data_extracted"] = len(search_result.jobs)
                
                # æº–å‚™åŸå§‹æ•¸æ“š
                raw_data = {
                    "search_query": self.test_query,
                    "search_location": self.test_location,
                    "timestamp": datetime.now().isoformat(),
                    "platform": "seek",
                    "total_found": search_result.total_count,
                    "jobs": search_result.jobs,
                    "metadata": {
                        "method_used": search_result.method_used.value,
                        "execution_time": search_result.execution_time,
                        "page": search_result.page,
                        "has_next_page": search_result.has_next_page
                    }
                }
                
                # ç”Ÿæˆæ–‡ä»¶è·¯å¾‘
                file_path = f"seek/{datetime.now().strftime('%Y%m%d')}/{self.test_query.replace(' ', '_')}_{self.timestamp}.raw"
                
                # å˜—è©¦å­˜å„²åˆ°MinIO
                if self.minio_client and self.storage_service:
                    try:
                        raw_data_bytes = json.dumps(raw_data, ensure_ascii=False, indent=2).encode('utf-8')
                        stored_path = await self.storage_service.store_raw_data(
                            "seek",
                            self.test_query,
                            raw_data_bytes,
                            {
                                "location": self.test_location,
                                "job_count": len(search_result.jobs)
                            }
                        )
                        
                        stage_result["minio_stored"] = True
                        stage_result["files_created"].append(stored_path)
                        self.logger.info(f"åŸå§‹æ•¸æ“šå·²å­˜å„²åˆ°MinIO: {stored_path}")
                        
                    except Exception as e:
                        self.logger.error(f"MinIOå­˜å„²å¤±æ•—: {str(e)}")
                        stage_result["errors"].append(f"MinIOå­˜å„²å¤±æ•—: {str(e)}")
                
                # æœ¬åœ°å‚™ä»½å­˜å„²
                local_dir = Path("./test_output/raw_data")
                local_dir.mkdir(parents=True, exist_ok=True)
                local_file = local_dir / f"seek_{self.timestamp}.json"
                
                with open(local_file, 'w', encoding='utf-8') as f:
                    json.dump(raw_data, f, ensure_ascii=False, indent=2)
                
                stage_result["files_created"].append(str(local_file))
                stage_result["success"] = True
                
                # æ›´æ–°ç‹€æ…‹
                self.etl_status["stage_1_raw_data"]["completed"] = True
                self.etl_status["stage_1_raw_data"]["files"] = stage_result["files_created"]
                self.etl_status["stage_1_raw_data"]["count"] = stage_result["data_extracted"]
                
                self.logger.info(f"éšæ®µ1å®Œæˆï¼šæŠ“å–äº†{stage_result['data_extracted']}å€‹è·ä½")
                
            else:
                error_msg = search_result.error_message or "æœªæ‰¾åˆ°ä»»ä½•è·ä½æ•¸æ“š"
                stage_result["errors"].append(error_msg)
                self.logger.error(f"æœç´¢å¤±æ•—: {error_msg}")
                
        except Exception as e:
            error_msg = f"éšæ®µ1åŸ·è¡Œå¤±æ•—: {str(e)}"
            stage_result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        finally:
            stage_result["end_time"] = datetime.now().isoformat()
        
        return stage_result
    
    async def run_stage_2_ai_processing(self) -> Dict[str, Any]:
        """éšæ®µ2ï¼šAIè§£æè™•ç†
        
        Returns:
            Dict[str, Any]: éšæ®µåŸ·è¡Œçµæœ
        """
        stage_result = {
            "stage": "AIè§£æè™•ç†",
            "success": False,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "jobs_processed": 0,
            "files_created": [],
            "ai_model_used": "æ¨¡æ“¬AIè™•ç†",
            "errors": []
        }
        
        try:
            self.logger.info("é–‹å§‹éšæ®µ2ï¼šAIè§£æè™•ç†")
            
            # æª¢æŸ¥éšæ®µ1æ˜¯å¦å®Œæˆ
            if not self.etl_status["stage_1_raw_data"]["completed"]:
                raise Exception("éšæ®µ1æœªå®Œæˆï¼Œç„¡æ³•é€²è¡ŒAIè™•ç†")
            
            # è®€å–åŸå§‹æ•¸æ“š
            raw_files = self.etl_status["stage_1_raw_data"]["files"]
            if not raw_files:
                raise Exception("æœªæ‰¾åˆ°åŸå§‹æ•¸æ“šæ–‡ä»¶")
            
            # è™•ç†æœ¬åœ°æ–‡ä»¶ï¼ˆä½œç‚ºç¤ºä¾‹ï¼‰
            local_raw_file = None
            for file_path in raw_files:
                if file_path.endswith('.json') and 'test_output' in file_path:
                    local_raw_file = file_path
                    break
            
            if not local_raw_file or not Path(local_raw_file).exists():
                raise Exception("æœªæ‰¾åˆ°æœ¬åœ°åŸå§‹æ•¸æ“šæ–‡ä»¶")
            
            # è®€å–åŸå§‹æ•¸æ“š
            with open(local_raw_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            # æ¨¡æ“¬AIè™•ç†ï¼ˆå¯¦éš›æ‡‰è©²èª¿ç”¨OpenAI APIï¼‰
            processed_jobs = []
            for job in raw_data.get('jobs', []):
                # æ¨¡æ“¬AIå¢å¼·è™•ç†
                processed_job = {
                    "id": job.get('id', f"seek_{len(processed_jobs)+1}"),
                    "title": job.get('title', '').strip(),
                    "company": job.get('company', '').strip(),
                    "location": job.get('location', '').strip(),
                    "description": job.get('description', '').strip(),
                    "salary": job.get('salary', ''),
                    "job_type": job.get('job_type', ''),
                    "posted_date": job.get('posted_date', ''),
                    "url": job.get('url', ''),
                    "ai_analysis": {
                        "skills_extracted": ["Python", "JavaScript", "SQL"],  # æ¨¡æ“¬æŠ€èƒ½æå–
                        "experience_level": "Mid-level",  # æ¨¡æ“¬ç¶“é©—ç­‰ç´šåˆ†æ
                        "remote_friendly": False,  # æ¨¡æ“¬é ç¨‹å·¥ä½œåˆ†æ
                        "confidence_score": 0.85  # æ¨¡æ“¬ç½®ä¿¡åº¦
                    },
                    "processing_metadata": {
                        "processed_at": datetime.now().isoformat(),
                        "ai_model": "gpt-4-vision-preview",
                        "processing_version": "1.0"
                    }
                }
                processed_jobs.append(processed_job)
            
            # æº–å‚™AIè™•ç†å¾Œçš„æ•¸æ“š
            ai_processed_data = {
                "source_file": local_raw_file,
                "processing_timestamp": datetime.now().isoformat(),
                "ai_model": "gpt-4-vision-preview",
                "jobs_processed": len(processed_jobs),
                "jobs": processed_jobs,
                "processing_stats": {
                    "total_jobs": len(processed_jobs),
                    "successfully_processed": len(processed_jobs),
                    "failed_processing": 0,
                    "average_confidence": 0.85
                }
            }
            
            # å­˜å„²AIè™•ç†å¾Œçš„æ•¸æ“š
            ai_dir = Path("./test_output/ai_processed")
            ai_dir.mkdir(parents=True, exist_ok=True)
            ai_file = ai_dir / f"seek_ai_processed_{self.timestamp}.json"
            
            with open(ai_file, 'w', encoding='utf-8') as f:
                json.dump(ai_processed_data, f, ensure_ascii=False, indent=2)
            
            stage_result["jobs_processed"] = len(processed_jobs)
            stage_result["files_created"].append(str(ai_file))
            stage_result["success"] = True
            
            # å˜—è©¦å­˜å„²åˆ°MinIO
            if self.minio_client and self.storage_service:
                try:
                    ai_stored_path = await self.storage_service.store_ai_processed_data(
                        local_raw_file,
                        ai_processed_data,
                        "gpt-4-vision-preview",
                        {"test_run": True}
                    )
                    stage_result["files_created"].append(ai_stored_path)
                    self.logger.info(f"AIè™•ç†æ•¸æ“šå·²å­˜å„²åˆ°MinIO: {ai_stored_path}")
                except Exception as e:
                    self.logger.warning(f"MinIOå­˜å„²å¤±æ•—: {str(e)}")
            
            # æ›´æ–°ç‹€æ…‹
            self.etl_status["stage_2_ai_processed"]["completed"] = True
            self.etl_status["stage_2_ai_processed"]["files"] = stage_result["files_created"]
            self.etl_status["stage_2_ai_processed"]["count"] = stage_result["jobs_processed"]
            
            self.logger.info(f"éšæ®µ2å®Œæˆï¼šAIè™•ç†äº†{stage_result['jobs_processed']}å€‹è·ä½")
            
        except Exception as e:
            error_msg = f"éšæ®µ2åŸ·è¡Œå¤±æ•—: {str(e)}"
            stage_result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        finally:
            stage_result["end_time"] = datetime.now().isoformat()
        
        return stage_result
    
    async def run_stage_3_data_cleaning(self) -> Dict[str, Any]:
        """éšæ®µ3ï¼šæ•¸æ“šæ¸…ç†
        
        Returns:
            Dict[str, Any]: éšæ®µåŸ·è¡Œçµæœ
        """
        stage_result = {
            "stage": "æ•¸æ“šæ¸…ç†",
            "success": False,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "jobs_cleaned": 0,
            "duplicates_removed": 0,
            "files_created": [],
            "errors": []
        }
        
        try:
            self.logger.info("é–‹å§‹éšæ®µ3ï¼šæ•¸æ“šæ¸…ç†")
            
            # æª¢æŸ¥éšæ®µ2æ˜¯å¦å®Œæˆ
            if not self.etl_status["stage_2_ai_processed"]["completed"]:
                raise Exception("éšæ®µ2æœªå®Œæˆï¼Œç„¡æ³•é€²è¡Œæ•¸æ“šæ¸…ç†")
            
            # è®€å–AIè™•ç†å¾Œçš„æ•¸æ“š
            ai_files = self.etl_status["stage_2_ai_processed"]["files"]
            local_ai_file = None
            for file_path in ai_files:
                if file_path.endswith('.json') and 'test_output' in file_path:
                    local_ai_file = file_path
                    break
            
            if not local_ai_file or not Path(local_ai_file).exists():
                raise Exception("æœªæ‰¾åˆ°AIè™•ç†å¾Œçš„æ•¸æ“šæ–‡ä»¶")
            
            # è®€å–AIè™•ç†å¾Œçš„æ•¸æ“š
            with open(local_ai_file, 'r', encoding='utf-8') as f:
                ai_data = json.load(f)
            
            # æ•¸æ“šæ¸…ç†è™•ç†
            cleaned_jobs = []
            seen_jobs = set()  # ç”¨æ–¼å»é‡
            
            for job in ai_data.get('jobs', []):
                # æ•¸æ“šæ¨™æº–åŒ–
                cleaned_job = {
                    "id": job.get('id', '').strip(),
                    "title": self._clean_text(job.get('title', '')),
                    "company": self._clean_text(job.get('company', '')),
                    "location": self._standardize_location(job.get('location', '')),
                    "description": self._clean_description(job.get('description', '')),
                    "salary": self._standardize_salary(job.get('salary', '')),
                    "job_type": self._standardize_job_type(job.get('job_type', '')),
                    "posted_date": self._standardize_date(job.get('posted_date', '')),
                    "url": job.get('url', '').strip(),
                    "skills": job.get('ai_analysis', {}).get('skills_extracted', []),
                    "experience_level": job.get('ai_analysis', {}).get('experience_level', ''),
                    "remote_friendly": job.get('ai_analysis', {}).get('remote_friendly', False),
                    "data_quality": {
                        "completeness_score": self._calculate_completeness(job),
                        "confidence_score": job.get('ai_analysis', {}).get('confidence_score', 0.0),
                        "cleaned_at": datetime.now().isoformat()
                    }
                }
                
                # å»é‡æª¢æŸ¥
                job_signature = f"{cleaned_job['title']}_{cleaned_job['company']}_{cleaned_job['location']}"
                if job_signature not in seen_jobs:
                    seen_jobs.add(job_signature)
                    cleaned_jobs.append(cleaned_job)
                else:
                    stage_result["duplicates_removed"] += 1
            
            # æº–å‚™æ¸…ç†å¾Œçš„æ•¸æ“š
            cleaned_data = {
                "source_file": local_ai_file,
                "cleaning_timestamp": datetime.now().isoformat(),
                "cleaning_version": "1.0",
                "jobs_input": len(ai_data.get('jobs', [])),
                "jobs_output": len(cleaned_jobs),
                "duplicates_removed": stage_result["duplicates_removed"],
                "jobs": cleaned_jobs,
                "cleaning_stats": {
                    "total_processed": len(cleaned_jobs),
                    "average_completeness": sum(job['data_quality']['completeness_score'] for job in cleaned_jobs) / len(cleaned_jobs) if cleaned_jobs else 0,
                    "high_quality_jobs": len([job for job in cleaned_jobs if job['data_quality']['completeness_score'] > 0.8])
                }
            }
            
            # å­˜å„²æ¸…ç†å¾Œçš„æ•¸æ“š
            cleaned_dir = Path("./test_output/cleaned_data")
            cleaned_dir.mkdir(parents=True, exist_ok=True)
            cleaned_file = cleaned_dir / f"seek_cleaned_{self.timestamp}.json"
            
            with open(cleaned_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
            
            stage_result["jobs_cleaned"] = len(cleaned_jobs)
            stage_result["files_created"].append(str(cleaned_file))
            stage_result["success"] = True
            
            # å˜—è©¦å­˜å„²åˆ°MinIO
            if self.minio_client and self.storage_service:
                try:
                    cleaned_stored_path = await self.storage_service.store_cleaned_data(
                        local_ai_file,
                        cleaned_data,
                        {"test_run": True}
                    )
                    stage_result["files_created"].append(cleaned_stored_path)
                    self.logger.info(f"æ¸…ç†æ•¸æ“šå·²å­˜å„²åˆ°MinIO: {cleaned_stored_path}")
                except Exception as e:
                    self.logger.warning(f"MinIOå­˜å„²å¤±æ•—: {str(e)}")
            
            # æ›´æ–°ç‹€æ…‹
            self.etl_status["stage_3_cleaned_data"]["completed"] = True
            self.etl_status["stage_3_cleaned_data"]["files"] = stage_result["files_created"]
            self.etl_status["stage_3_cleaned_data"]["count"] = stage_result["jobs_cleaned"]
            
            self.logger.info(f"éšæ®µ3å®Œæˆï¼šæ¸…ç†äº†{stage_result['jobs_cleaned']}å€‹è·ä½ï¼Œç§»é™¤äº†{stage_result['duplicates_removed']}å€‹é‡è¤‡é …")
            
        except Exception as e:
            error_msg = f"éšæ®µ3åŸ·è¡Œå¤±æ•—: {str(e)}"
            stage_result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        finally:
            stage_result["end_time"] = datetime.now().isoformat()
        
        return stage_result
    
    async def run_stage_4_database_loading(self) -> Dict[str, Any]:
        """éšæ®µ4ï¼šæ•¸æ“šåº«è¼‰å…¥
        
        Returns:
            Dict[str, Any]: éšæ®µåŸ·è¡Œçµæœ
        """
        stage_result = {
            "stage": "æ•¸æ“šåº«è¼‰å…¥",
            "success": False,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "records_loaded": 0,
            "database_used": "æ¨¡æ“¬æ•¸æ“šåº«",
            "errors": []
        }
        
        try:
            self.logger.info("é–‹å§‹éšæ®µ4ï¼šæ•¸æ“šåº«è¼‰å…¥")
            
            # æª¢æŸ¥éšæ®µ3æ˜¯å¦å®Œæˆ
            if not self.etl_status["stage_3_cleaned_data"]["completed"]:
                raise Exception("éšæ®µ3æœªå®Œæˆï¼Œç„¡æ³•é€²è¡Œæ•¸æ“šåº«è¼‰å…¥")
            
            # è®€å–æ¸…ç†å¾Œçš„æ•¸æ“š
            cleaned_files = self.etl_status["stage_3_cleaned_data"]["files"]
            local_cleaned_file = None
            for file_path in cleaned_files:
                if file_path.endswith('.json') and 'test_output' in file_path:
                    local_cleaned_file = file_path
                    break
            
            if not local_cleaned_file or not Path(local_cleaned_file).exists():
                raise Exception("æœªæ‰¾åˆ°æ¸…ç†å¾Œçš„æ•¸æ“šæ–‡ä»¶")
            
            # è®€å–æ¸…ç†å¾Œçš„æ•¸æ“š
            with open(local_cleaned_file, 'r', encoding='utf-8') as f:
                cleaned_data = json.load(f)
            
            # æ¨¡æ“¬æ•¸æ“šåº«è¼‰å…¥ï¼ˆå¯¦éš›æ‡‰è©²é€£æ¥PostgreSQLï¼‰
            jobs_to_load = cleaned_data.get('jobs', [])
            
            # å‰µå»ºæ¨¡æ“¬æ•¸æ“šåº«è¨˜éŒ„æ–‡ä»¶
            db_dir = Path("./test_output/database")
            db_dir.mkdir(parents=True, exist_ok=True)
            db_file = db_dir / f"seek_db_records_{self.timestamp}.json"
            
            # æ¨¡æ“¬æ•¸æ“šåº«è¨˜éŒ„æ ¼å¼
            db_records = []
            for i, job in enumerate(jobs_to_load, 1):
                db_record = {
                    "id": i,
                    "external_id": job.get('id', f"seek_{i}"),
                    "title": job.get('title', ''),
                    "company": job.get('company', ''),
                    "location": job.get('location', ''),
                    "description": job.get('description', ''),
                    "salary_range": job.get('salary', ''),
                    "job_type": job.get('job_type', ''),
                    "posted_date": job.get('posted_date', ''),
                    "url": job.get('url', ''),
                    "skills": job.get('skills', []),
                    "experience_level": job.get('experience_level', ''),
                    "remote_friendly": job.get('remote_friendly', False),
                    "platform": "seek",
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat(),
                    "data_quality_score": job.get('data_quality', {}).get('completeness_score', 0.0)
                }
                db_records.append(db_record)
            
            # ä¿å­˜æ¨¡æ“¬æ•¸æ“šåº«è¨˜éŒ„
            db_data = {
                "table": "job_listings",
                "timestamp": datetime.now().isoformat(),
                "source_file": local_cleaned_file,
                "records_count": len(db_records),
                "records": db_records
            }
            
            with open(db_file, 'w', encoding='utf-8') as f:
                json.dump(db_data, f, ensure_ascii=False, indent=2)
            
            stage_result["records_loaded"] = len(db_records)
            stage_result["success"] = True
            
            # æ›´æ–°ç‹€æ…‹
            self.etl_status["stage_4_db_loaded"]["completed"] = True
            self.etl_status["stage_4_db_loaded"]["records"] = stage_result["records_loaded"]
            
            self.logger.info(f"éšæ®µ4å®Œæˆï¼šè¼‰å…¥äº†{stage_result['records_loaded']}æ¢è¨˜éŒ„åˆ°æ•¸æ“šåº«")
            
        except Exception as e:
            error_msg = f"éšæ®µ4åŸ·è¡Œå¤±æ•—: {str(e)}"
            stage_result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        finally:
            stage_result["end_time"] = datetime.now().isoformat()
        
        return stage_result
    
    async def run_stage_5_csv_export(self) -> Dict[str, Any]:
        """éšæ®µ5ï¼šCSVå°å‡º
        
        Returns:
            Dict[str, Any]: éšæ®µåŸ·è¡Œçµæœ
        """
        stage_result = {
            "stage": "CSVå°å‡º",
            "success": False,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "records_exported": 0,
            "files_created": [],
            "errors": []
        }
        
        try:
            self.logger.info("é–‹å§‹éšæ®µ5ï¼šCSVå°å‡º")
            
            # æª¢æŸ¥éšæ®µ4æ˜¯å¦å®Œæˆ
            if not self.etl_status["stage_4_db_loaded"]["completed"]:
                raise Exception("éšæ®µ4æœªå®Œæˆï¼Œç„¡æ³•é€²è¡ŒCSVå°å‡º")
            
            # è®€å–æ•¸æ“šåº«è¨˜éŒ„ï¼ˆå¾æ¨¡æ“¬æ–‡ä»¶ï¼‰
            db_dir = Path("./test_output/database")
            db_files = list(db_dir.glob(f"seek_db_records_{self.timestamp}.json"))
            
            if not db_files:
                raise Exception("æœªæ‰¾åˆ°æ•¸æ“šåº«è¨˜éŒ„æ–‡ä»¶")
            
            db_file = db_files[0]
            with open(db_file, 'r', encoding='utf-8') as f:
                db_data = json.load(f)
            
            records = db_data.get('records', [])
            
            # å‰µå»ºCSVå°å‡ºç›®éŒ„
            csv_dir = Path("./test_output/csv_exports")
            csv_dir.mkdir(parents=True, exist_ok=True)
            
            # å°å‡ºä¸»è¦è·ä½æ•¸æ“šCSV
            main_csv_file = csv_dir / f"seek_jobs_{self.timestamp}.csv"
            
            with open(main_csv_file, 'w', newline='', encoding='utf-8') as csvfile:
                if records:
                    fieldnames = [
                        'id', 'external_id', 'title', 'company', 'location', 
                        'salary_range', 'job_type', 'posted_date', 'url', 
                        'experience_level', 'remote_friendly', 'platform', 
                        'created_at', 'data_quality_score'
                    ]
                    
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for record in records:
                        # æº–å‚™CSVè¡Œæ•¸æ“š
                        csv_row = {field: record.get(field, '') for field in fieldnames}
                        # è™•ç†å¸ƒçˆ¾å€¼
                        csv_row['remote_friendly'] = 'Yes' if record.get('remote_friendly') else 'No'
                        # è™•ç†æŠ€èƒ½åˆ—è¡¨
                        if 'skills' in record and record['skills']:
                            csv_row['skills'] = ', '.join(record['skills'])
                        
                        writer.writerow(csv_row)
            
            stage_result["files_created"].append(str(main_csv_file))
            
            # å°å‡ºæŠ€èƒ½çµ±è¨ˆCSV
            skills_csv_file = csv_dir / f"seek_skills_stats_{self.timestamp}.csv"
            skills_count = {}
            
            for record in records:
                for skill in record.get('skills', []):
                    skills_count[skill] = skills_count.get(skill, 0) + 1
            
            with open(skills_csv_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['Skill', 'Count', 'Percentage'])
                
                total_jobs = len(records)
                for skill, count in sorted(skills_count.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_jobs * 100) if total_jobs > 0 else 0
                    writer.writerow([skill, count, f"{percentage:.1f}%"])
            
            stage_result["files_created"].append(str(skills_csv_file))
            
            # å°å‡ºå…¬å¸çµ±è¨ˆCSV
            company_csv_file = csv_dir / f"seek_companies_{self.timestamp}.csv"
            company_count = {}
            
            for record in records:
                company = record.get('company', 'Unknown')
                company_count[company] = company_count.get(company, 0) + 1
            
            with open(company_csv_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['Company', 'Job_Count'])
                
                for company, count in sorted(company_count.items(), key=lambda x: x[1], reverse=True):
                    writer.writerow([company, count])
            
            stage_result["files_created"].append(str(company_csv_file))
            
            stage_result["records_exported"] = len(records)
            stage_result["success"] = True
            
            # æ›´æ–°ç‹€æ…‹
            self.etl_status["stage_5_csv_exported"]["completed"] = True
            self.etl_status["stage_5_csv_exported"]["files"] = stage_result["files_created"]
            self.etl_status["stage_5_csv_exported"]["count"] = stage_result["records_exported"]
            
            self.logger.info(f"éšæ®µ5å®Œæˆï¼šå°å‡ºäº†{stage_result['records_exported']}æ¢è¨˜éŒ„åˆ°{len(stage_result['files_created'])}å€‹CSVæ–‡ä»¶")
            
        except Exception as e:
            error_msg = f"éšæ®µ5åŸ·è¡Œå¤±æ•—: {str(e)}"
            stage_result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        finally:
            stage_result["end_time"] = datetime.now().isoformat()
        
        return stage_result
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬æ•¸æ“š"""
        if not text:
            return ""
        return text.strip().replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    
    def _standardize_location(self, location: str) -> str:
        """æ¨™æº–åŒ–åœ°é»æ•¸æ“š"""
        if not location:
            return ""
        
        location = self._clean_text(location)
        # ç°¡å–®çš„åœ°é»æ¨™æº–åŒ–
        location_mapping = {
            "sydney nsw": "Sydney, NSW",
            "melbourne vic": "Melbourne, VIC",
            "brisbane qld": "Brisbane, QLD",
            "perth wa": "Perth, WA",
            "adelaide sa": "Adelaide, SA"
        }
        
        location_lower = location.lower()
        for key, value in location_mapping.items():
            if key in location_lower:
                return value
        
        return location
    
    def _clean_description(self, description: str) -> str:
        """æ¸…ç†è·ä½æè¿°"""
        if not description:
            return ""
        
        # ç§»é™¤HTMLæ¨™ç±¤å’Œå¤šé¤˜ç©ºç™½
        import re
        description = re.sub(r'<[^>]+>', '', description)
        description = re.sub(r'\s+', ' ', description)
        return description.strip()
    
    def _standardize_salary(self, salary: str) -> str:
        """æ¨™æº–åŒ–è–ªè³‡æ•¸æ“š"""
        if not salary:
            return ""
        
        salary = self._clean_text(salary)
        # ç°¡å–®çš„è–ªè³‡æ¨™æº–åŒ–
        if 'k' in salary.lower() or '$' in salary:
            return salary
        return ""
    
    def _standardize_job_type(self, job_type: str) -> str:
        """æ¨™æº–åŒ–å·¥ä½œé¡å‹"""
        if not job_type:
            return "Full-time"  # é»˜èªå€¼
        
        job_type = self._clean_text(job_type).lower()
        
        type_mapping = {
            "full time": "Full-time",
            "full-time": "Full-time",
            "part time": "Part-time",
            "part-time": "Part-time",
            "contract": "Contract",
            "casual": "Casual",
            "temporary": "Temporary",
            "temp": "Temporary"
        }
        
        for key, value in type_mapping.items():
            if key in job_type:
                return value
        
        return "Full-time"
    
    def _standardize_date(self, date_str: str) -> str:
        """æ¨™æº–åŒ–æ—¥æœŸæ ¼å¼"""
        if not date_str:
            return ""
        
        # ç°¡å–®çš„æ—¥æœŸè™•ç†
        date_str = self._clean_text(date_str)
        if 'ago' in date_str.lower():
            # è™•ç†ç›¸å°æ—¥æœŸ
            return datetime.now().strftime('%Y-%m-%d')
        
        return date_str
    
    def _calculate_completeness(self, job: Dict[str, Any]) -> float:
        """è¨ˆç®—æ•¸æ“šå®Œæ•´æ€§åˆ†æ•¸"""
        required_fields = ['title', 'company', 'location', 'description']
        optional_fields = ['salary', 'job_type', 'posted_date', 'url']
        
        required_score = sum(1 for field in required_fields if job.get(field, '').strip())
        optional_score = sum(0.5 for field in optional_fields if job.get(field, '').strip())
        
        max_score = len(required_fields) + len(optional_fields) * 0.5
        return (required_score + optional_score) / max_score
    
    async def run_complete_etl_pipeline(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„ETL pipeline
        
        Returns:
            Dict[str, Any]: å®Œæ•´çš„æ¸¬è©¦çµæœ
        """
        pipeline_result = {
            "pipeline_name": "Seek ETL Pipeline",
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "overall_success": False,
            "stages_completed": 0,
            "total_stages": 5,
            "stage_results": {},
            "file_locations": {},
            "summary": {},
            "errors": []
        }
        
        try:
            self.logger.info("é–‹å§‹é‹è¡Œå®Œæ•´çš„Seek ETL Pipeline")
            
            # åˆå§‹åŒ–çµ„ä»¶
            if not await self.initialize_components():
                raise Exception("çµ„ä»¶åˆå§‹åŒ–å¤±æ•—")
            
            # éšæ®µ1ï¼šåŸå§‹æ•¸æ“šæŠ“å–
            stage1_result = await self.run_stage_1_raw_data_extraction()
            pipeline_result["stage_results"]["stage_1"] = stage1_result
            if stage1_result["success"]:
                pipeline_result["stages_completed"] += 1
            else:
                pipeline_result["errors"].extend(stage1_result["errors"])
            
            # éšæ®µ2ï¼šAIè§£æè™•ç†
            stage2_result = await self.run_stage_2_ai_processing()
            pipeline_result["stage_results"]["stage_2"] = stage2_result
            if stage2_result["success"]:
                pipeline_result["stages_completed"] += 1
            else:
                pipeline_result["errors"].extend(stage2_result["errors"])
            
            # éšæ®µ3ï¼šæ•¸æ“šæ¸…ç†
            stage3_result = await self.run_stage_3_data_cleaning()
            pipeline_result["stage_results"]["stage_3"] = stage3_result
            if stage3_result["success"]:
                pipeline_result["stages_completed"] += 1
            else:
                pipeline_result["errors"].extend(stage3_result["errors"])
            
            # éšæ®µ4ï¼šæ•¸æ“šåº«è¼‰å…¥
            stage4_result = await self.run_stage_4_database_loading()
            pipeline_result["stage_results"]["stage_4"] = stage4_result
            if stage4_result["success"]:
                pipeline_result["stages_completed"] += 1
            else:
                pipeline_result["errors"].extend(stage4_result["errors"])
            
            # éšæ®µ5ï¼šCSVå°å‡º
            stage5_result = await self.run_stage_5_csv_export()
            pipeline_result["stage_results"]["stage_5"] = stage5_result
            if stage5_result["success"]:
                pipeline_result["stages_completed"] += 1
            else:
                pipeline_result["errors"].extend(stage5_result["errors"])
            
            # æª¢æŸ¥æ•´é«”æˆåŠŸ
            pipeline_result["overall_success"] = pipeline_result["stages_completed"] == pipeline_result["total_stages"]
            
            # ç”Ÿæˆæ–‡ä»¶ä½ç½®å ±å‘Š
            pipeline_result["file_locations"] = {
                "raw_data": self.etl_status["stage_1_raw_data"]["files"],
                "ai_processed": self.etl_status["stage_2_ai_processed"]["files"],
                "cleaned_data": self.etl_status["stage_3_cleaned_data"]["files"],
                "csv_exports": self.etl_status["stage_5_csv_exported"]["files"]
            }
            
            # ç”Ÿæˆæ‘˜è¦
            pipeline_result["summary"] = {
                "jobs_extracted": self.etl_status["stage_1_raw_data"]["count"],
                "jobs_ai_processed": self.etl_status["stage_2_ai_processed"]["count"],
                "jobs_cleaned": self.etl_status["stage_3_cleaned_data"]["count"],
                "records_in_db": self.etl_status["stage_4_db_loaded"]["records"],
                "records_exported": self.etl_status["stage_5_csv_exported"]["count"]
            }
            
            self.logger.info(f"ETL Pipelineå®Œæˆï¼š{pipeline_result['stages_completed']}/{pipeline_result['total_stages']}å€‹éšæ®µæˆåŠŸ")
            
        except Exception as e:
            error_msg = f"ETL PipelineåŸ·è¡Œå¤±æ•—: {str(e)}"
            pipeline_result["errors"].append(error_msg)
            self.logger.error(error_msg)
        
        finally:
            pipeline_result["end_time"] = datetime.now().isoformat()
        
        return pipeline_result
    
    def generate_test_report(self, pipeline_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        
        Args:
            pipeline_result: PipelineåŸ·è¡Œçµæœ
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ¸¬è©¦å ±å‘Š
        """
        report = []
        report.append("="*80)
        report.append("Seekçˆ¬èŸ²ETL Pipelineæ¸¬è©¦å ±å‘Š")
        report.append("="*80)
        report.append(f"æ¸¬è©¦æ™‚é–“: {pipeline_result['start_time']} - {pipeline_result['end_time']}")
        report.append(f"æ•´é«”çµæœ: {'âœ… æˆåŠŸ' if pipeline_result['overall_success'] else 'âŒ å¤±æ•—'}")
        report.append(f"å®Œæˆéšæ®µ: {pipeline_result['stages_completed']}/{pipeline_result['total_stages']}")
        report.append("")
        
        # å„éšæ®µè©³æƒ…
        report.append("éšæ®µåŸ·è¡Œè©³æƒ…:")
        report.append("-"*50)
        
        stage_names = {
            "stage_1": "éšæ®µ1: åŸå§‹æ•¸æ“šæŠ“å–",
            "stage_2": "éšæ®µ2: AIè§£æè™•ç†",
            "stage_3": "éšæ®µ3: æ•¸æ“šæ¸…ç†",
            "stage_4": "éšæ®µ4: æ•¸æ“šåº«è¼‰å…¥",
            "stage_5": "éšæ®µ5: CSVå°å‡º"
        }
        
        for stage_key, stage_name in stage_names.items():
            if stage_key in pipeline_result["stage_results"]:
                stage_result = pipeline_result["stage_results"][stage_key]
                status = "âœ… æˆåŠŸ" if stage_result["success"] else "âŒ å¤±æ•—"
                report.append(f"{stage_name}: {status}")
                
                if stage_result["errors"]:
                    for error in stage_result["errors"]:
                        report.append(f"  éŒ¯èª¤: {error}")
        
        report.append("")
        
        # æ•¸æ“šçµ±è¨ˆ
        if "summary" in pipeline_result:
            summary = pipeline_result["summary"]
            report.append("æ•¸æ“šè™•ç†çµ±è¨ˆ:")
            report.append("-"*30)
            report.append(f"åŸå§‹æ•¸æ“šæŠ“å–: {summary.get('jobs_extracted', 0)} å€‹è·ä½")
            report.append(f"AIè™•ç†å®Œæˆ: {summary.get('jobs_ai_processed', 0)} å€‹è·ä½")
            report.append(f"æ•¸æ“šæ¸…ç†å®Œæˆ: {summary.get('jobs_cleaned', 0)} å€‹è·ä½")
            report.append(f"æ•¸æ“šåº«è¼‰å…¥: {summary.get('records_in_db', 0)} æ¢è¨˜éŒ„")
            report.append(f"CSVå°å‡º: {summary.get('records_exported', 0)} æ¢è¨˜éŒ„")
            report.append("")
        
        # æ–‡ä»¶ä½ç½®
        if "file_locations" in pipeline_result:
            locations = pipeline_result["file_locations"]
            report.append("æ–‡ä»¶å­˜æ”¾ä½ç½®:")
            report.append("-"*40)
            
            for category, files in locations.items():
                if files:
                    report.append(f"{category}:")
                    for file_path in files:
                        report.append(f"  - {file_path}")
            report.append("")
        
        # éŒ¯èª¤æ‘˜è¦
        if pipeline_result["errors"]:
            report.append("éŒ¯èª¤æ‘˜è¦:")
            report.append("-"*20)
            for error in pipeline_result["errors"]:
                report.append(f"- {error}")
            report.append("")
        
        report.append("="*80)
        
        return "\n".join(report)


async def main():
    """ä¸»å‡½æ•¸ - é‹è¡ŒSeek ETLæ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹Seekçˆ¬èŸ²ETL Pipelineå¯¦éš›æ¸¬è©¦...")
    print("")
    
    # å‰µå»ºETLé‹è¡Œå™¨
    runner = SeekETLRunner()
    
    try:
        # é‹è¡Œå®Œæ•´çš„ETL pipeline
        results = await runner.run_complete_etl_pipeline()
        
        # ç”Ÿæˆä¸¦é¡¯ç¤ºæ¸¬è©¦å ±å‘Š
        report = runner.generate_test_report(results)
        print(report)
        
        # ä¿å­˜æ¸¬è©¦å ±å‘Š
        report_dir = Path("./test_output")
        report_dir.mkdir(parents=True, exist_ok=True)
        report_file = report_dir / f"seek_etl_test_report_{runner.timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æª¢æŸ¥æ¸¬è©¦çµæœ
        if results["overall_success"]:
            print("\nğŸ‰ æ‰€æœ‰ETLéšæ®µæ¸¬è©¦æˆåŠŸå®Œæˆï¼")
            return 0
        else:
            print("\nâŒ éƒ¨åˆ†ETLéšæ®µæ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯ã€‚")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {str(e)}")
        return 1


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    exit_code = asyncio.run(main())
    exit(exit_code)