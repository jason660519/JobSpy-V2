#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Seek ETL Pipeline å®Œæ•´æ¸¬è©¦è…³æœ¬

æ­¤è…³æœ¬ç”¨æ–¼æ¸¬è©¦ Seek å¹³å°çš„å®Œæ•´ ETL æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. æ•¸æ“šæŠ“å–
2. AI è§£æè™•ç†
3. æ•¸æ“šæ¸…ç†
4. æ•¸æ“šåº«è¼‰å…¥
5. CSV å°å‡º
"""

import asyncio
import logging
import sys
from pathlib import Path
import time
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from crawler_engine.platforms.seek import SeekETLProcessor, SeekConfig
from crawler_engine.configuration.ai_config import AIConfig
from crawler_engine.configuration.storage_config import StorageConfig
from crawler_engine.configuration.export_config import ExportConfig

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'seek_pipeline_test_{int(time.time())}.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


async def test_seek_etl_pipeline():
    """æ¸¬è©¦ Seek ETL Pipeline"""
    logger.info("=" * 60)
    logger.info("é–‹å§‹ Seek ETL Pipeline å®Œæ•´æ¸¬è©¦")
    logger.info("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®
        logger.info("æ­¥é©Ÿ 1: åˆå§‹åŒ–é…ç½®")
        
        # Seek å¹³å°é…ç½®
        seek_config = SeekConfig()
        
        # AI é…ç½®
        ai_config = AIConfig(
            openai_api_key="your-openai-api-key",  # è«‹æ›¿æ›ç‚ºå¯¦éš›çš„ API key
            model="gpt-3.5-turbo",
            max_tokens=1000,
            temperature=0.1
        )
        
        # å­˜å„²é…ç½®
        storage_config = StorageConfig(
            minio_endpoint="localhost:9000",
            minio_access_key="minioadmin",
            minio_secret_key="minioadmin",
            minio_secure=False,
            database_url="sqlite:///seek_jobs.db"
        )
        
        # å°å‡ºé…ç½®
        export_config = ExportConfig(
            output_dir="./exports",
            file_format="csv",
            include_headers=True
        )
        
        # 2. åˆå§‹åŒ– ETL è™•ç†å™¨
        logger.info("æ­¥é©Ÿ 2: åˆå§‹åŒ– ETL è™•ç†å™¨")
        etl_processor = SeekETLProcessor(
            seek_config=seek_config,
            ai_config=ai_config,
            storage_config=storage_config,
            export_config=export_config
        )
        
        # 3. åŸ·è¡Œå®Œæ•´ ETL æµç¨‹
        logger.info("æ­¥é©Ÿ 3: åŸ·è¡Œå®Œæ•´ ETL æµç¨‹")
        
        # æ¸¬è©¦æœç´¢åƒæ•¸
        search_params = {
            'keywords': 'python developer',
            'location': 'Sydney',
            'limit': 10  # é™åˆ¶æ•¸é‡ä»¥ä¾¿æ¸¬è©¦
        }
        
        start_time = time.time()
        
        # åŸ·è¡Œ ETL æµç¨‹
        results = await etl_processor.run_etl_pipeline(
            search_params=search_params,
            enable_ai_processing=True,
            batch_size=5
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # 4. é©—è­‰çµæœ
        logger.info("æ­¥é©Ÿ 4: é©—è­‰çµæœ")
        
        if results:
            logger.info(f"âœ… ETL æµç¨‹åŸ·è¡ŒæˆåŠŸï¼")
            logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
            logger.info(f"   - ç¸½è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
            logger.info(f"   - åŸå§‹æ•¸æ“š: {results.get('raw_jobs_count', 0)} æ¢")
            logger.info(f"   - AI è™•ç†: {results.get('ai_processed_count', 0)} æ¢")
            logger.info(f"   - æ¸…ç†å¾Œ: {results.get('cleaned_jobs_count', 0)} æ¢")
            logger.info(f"   - æ•¸æ“šåº«å­˜å„²: {results.get('stored_jobs_count', 0)} æ¢")
            logger.info(f"   - CSV å°å‡º: {results.get('csv_export_path', 'N/A')}")
            
            # æª¢æŸ¥æ–‡ä»¶å­˜æ”¾ä½ç½®
            logger.info(f"ğŸ“ æ–‡ä»¶å­˜æ”¾ä½ç½®:")
            if results.get('csv_export_path'):
                csv_path = Path(results['csv_export_path'])
                if csv_path.exists():
                    logger.info(f"   âœ… CSV æ–‡ä»¶: {csv_path.absolute()}")
                    logger.info(f"   ğŸ“ æ–‡ä»¶å¤§å°: {csv_path.stat().st_size} bytes")
                else:
                    logger.warning(f"   âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            
            # æª¢æŸ¥æ•¸æ“šåº«
            db_stats = await etl_processor.database_storage.get_stats()
            if db_stats:
                logger.info(f"   ğŸ“Š æ•¸æ“šåº«çµ±è¨ˆ: {db_stats}")
            
        else:
            logger.error("âŒ ETL æµç¨‹åŸ·è¡Œå¤±æ•—ï¼Œæœªè¿”å›çµæœ")
            return False
        
        # 5. æ¸…ç†æ¸¬è©¦æ•¸æ“šï¼ˆå¯é¸ï¼‰
        logger.info("æ­¥é©Ÿ 5: æ¸¬è©¦å®Œæˆ")
        logger.info("ğŸ’¡ æç¤º: å¦‚éœ€æ¸…ç†æ¸¬è©¦æ•¸æ“šï¼Œè«‹æ‰‹å‹•åˆªé™¤ç”Ÿæˆçš„æ–‡ä»¶")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ETL Pipeline æ¸¬è©¦å¤±æ•—: {e}")
        logger.exception("è©³ç´°éŒ¯èª¤ä¿¡æ¯:")
        return False


async def test_individual_stages():
    """æ¸¬è©¦å„å€‹ç¨ç«‹éšæ®µ"""
    logger.info("\n" + "=" * 60)
    logger.info("é–‹å§‹å„éšæ®µç¨ç«‹æ¸¬è©¦")
    logger.info("=" * 60)
    
    try:
        # åˆå§‹åŒ–é…ç½®ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        seek_config = SeekConfig()
        ai_config = AIConfig(openai_api_key="test-key")
        storage_config = StorageConfig()
        export_config = ExportConfig()
        
        etl_processor = SeekETLProcessor(
            seek_config=seek_config,
            ai_config=ai_config,
            storage_config=storage_config,
            export_config=export_config
        )
        
        # æ¸¬è©¦æ•¸æ“šæŠ“å–
        logger.info("ğŸ” æ¸¬è©¦éšæ®µ 1: æ•¸æ“šæŠ“å–")
        search_params = {'keywords': 'test', 'limit': 3}
        raw_jobs = await etl_processor._scrape_jobs(search_params)
        logger.info(f"   æŠ“å–åˆ° {len(raw_jobs)} æ¢åŸå§‹æ•¸æ“š")
        
        if raw_jobs:
            # æ¸¬è©¦ AI è™•ç†
            logger.info("ğŸ¤– æ¸¬è©¦éšæ®µ 2: AI è™•ç†")
            ai_jobs = await etl_processor._process_with_ai(raw_jobs[:2])  # åªè™•ç†å‰2æ¢
            logger.info(f"   AI è™•ç†äº† {len(ai_jobs)} æ¢æ•¸æ“š")
            
            # æ¸¬è©¦æ•¸æ“šæ¸…ç†
            logger.info("ğŸ§¹ æ¸¬è©¦éšæ®µ 3: æ•¸æ“šæ¸…ç†")
            cleaned_jobs = await etl_processor._clean_jobs(ai_jobs)
            logger.info(f"   æ¸…ç†å¾Œä¿ç•™ {len(cleaned_jobs)} æ¢æ•¸æ“š")
            
            # æ¸¬è©¦æ•¸æ“šåº«è¼‰å…¥
            logger.info("ğŸ’¾ æ¸¬è©¦éšæ®µ 4: æ•¸æ“šåº«è¼‰å…¥")
            if cleaned_jobs:
                stored_count = await etl_processor._load_to_database(cleaned_jobs)
                logger.info(f"   å­˜å„²äº† {stored_count} æ¢æ•¸æ“šåˆ°æ•¸æ“šåº«")
                
                # æ¸¬è©¦ CSV å°å‡º
                logger.info("ğŸ“„ æ¸¬è©¦éšæ®µ 5: CSV å°å‡º")
                csv_path = await etl_processor._export_to_csv(cleaned_jobs)
                logger.info(f"   CSV å°å‡ºåˆ°: {csv_path}")
        
        logger.info("âœ… å„éšæ®µç¨ç«‹æ¸¬è©¦å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å„éšæ®µæ¸¬è©¦å¤±æ•—: {e}")
        logger.exception("è©³ç´°éŒ¯èª¤ä¿¡æ¯:")
        return False


async def main():
    """ä¸»å‡½æ•¸"""
    logger.info(f"ğŸš€ Seek ETL Pipeline æ¸¬è©¦é–‹å§‹ - {datetime.now()}")
    
    # æ¸¬è©¦å®Œæ•´ ETL æµç¨‹
    success1 = await test_seek_etl_pipeline()
    
    # æ¸¬è©¦å„å€‹ç¨ç«‹éšæ®µ
    success2 = await test_individual_stages()
    
    # ç¸½çµ
    logger.info("\n" + "=" * 60)
    logger.info("æ¸¬è©¦ç¸½çµ")
    logger.info("=" * 60)
    logger.info(f"å®Œæ•´ ETL æµç¨‹: {'âœ… é€šé' if success1 else 'âŒ å¤±æ•—'}")
    logger.info(f"å„éšæ®µç¨ç«‹æ¸¬è©¦: {'âœ… é€šé' if success2 else 'âŒ å¤±æ•—'}")
    logger.info(f"ç¸½é«”çµæœ: {'ğŸ‰ å…¨éƒ¨é€šé' if success1 and success2 else 'âš ï¸ éƒ¨åˆ†å¤±æ•—'}")
    logger.info(f"æ¸¬è©¦çµæŸæ™‚é–“: {datetime.now()}")
    
    return success1 and success2


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    result = asyncio.run(main())
    sys.exit(0 if result else 1)